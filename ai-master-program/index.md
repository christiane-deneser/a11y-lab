# Neuro-Inclusive Accessibility Lab

This repository is my personal **a11y lab**: a collection of
small, focused examples of **accessible UI components**,  
**neuro-inclusive design patterns**, and **AI-assisted workflows**
for accessibility testing and documentation.

I use this space to explore:

- âœ… Practical WCAG 2.2 / EN 301 549 implementation
- ðŸ§  Cognitive and neuro-inclusive design (ADHD, autism, dyslexia, cognitive load)
- ðŸ¤– AI-assisted accessibility (GitHub Copilot, axe LLM, Accessibility Insights, ChatGPT)

---

## Structure

- `components/` â€“ small, self-contained accessible components  
  (e.g. button, visually hidden text, alerts, skip links)

- `patterns/` â€“ larger UX patterns  
  (e.g. modal dialog, form validation, navigation)

- `audits/` â€“ examples of accessibility and cognitive-a11y audits  
  using tools like axe-core, Accessibility Insights, and manual testing

- `docs/` â€“ concept notes on cognitive accessibility, neuro-inclusive design,
  and how AI fits into my workflow

- `experiments/` â€“ AI prompts, workflows, and prototypes
  for AI-assisted accessibility

---

## How to use this repo

- Browse `components/` to see **minimal, framework-agnostic HTML/CSS/ARIA examples**.
- Open each componentâ€™s `README.md` for **WCAG references, keyboard behavior, and notes on cognitive accessibility**.
- Read `docs/cognitive-accessibility.md` to understand my design lens.
- Look at `audits/` for examples of how I structure accessibility findings.

This repo is a work in progress as I continue my  
**Master of Neuro-Inclusive & AI-Adaptive Experience Design**.
